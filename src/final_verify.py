import onnx
import numpy as np
import onnxruntime as ort
import torch
import whisper
import os

def final_verification_check(onnx_path, pt_path):
    """
    Comprehensive verification of ONNX model conversion
    """
    print("üîç Starting Final Verification Check...")
    
    try:
        # 1. Check files
        print("\n1. Checking files:")
        if os.path.exists(pt_path):
            print("‚úì PyTorch model found")
        if os.path.exists(onnx_path):
            print("‚úì ONNX model found")
            
        # 2. Validate ONNX model
        print("\n2. Validating ONNX model structure:")
        model = onnx.load(onnx_path)
        onnx.checker.check_model(model)
        print("‚úì ONNX model structure is valid")
        
        # 3. Test inference
        print("\n3. Testing ONNX inference capability:")
        session = ort.InferenceSession(onnx_path)
        input_name = session.get_inputs()[0].name
        input_shape = session.get_inputs()[0].shape
        print(f"‚úì Input name: {input_name}")
        print(f"‚úì Expected input shape: {input_shape}")
        
        # 4. Compare with PyTorch
        print("\n4. Preparing comparison test:")
        checkpoint = torch.load(pt_path, map_location=torch.device('cpu'))
        dims = checkpoint['dims']
        
        print("\n5. Running comparison test:")
        test_input = np.random.randn(1, dims['n_mels'], dims['n_audio_ctx'] * 2).astype(np.float32)
        ort_outputs = session.run(None, {input_name: test_input})
        
        base_model = whisper.load_model("tiny")
        base_model.load_state_dict(checkpoint['model_state_dict'])
        base_model.eval()
        
        with torch.no_grad():
            torch_input = torch.from_numpy(test_input)
            torch_output = base_model.encoder(torch_input)
        
        max_diff = np.max(np.abs(ort_outputs[0] - torch_output.numpy()))
        
        # 6. Check model size
        print("\n6. Model size analysis:")
        model_size = os.path.getsize(onnx_path) / (1024 * 1024)  # Convert to MB
        print(f"ONNX model size: {model_size:.2f}MB")
        
        # 7. Verify input processing
        print("\n7. Input processing verification:")
        test_small = np.random.randn(1, 80, 3000).astype(np.float32)
        _ = session.run(None, {input_name: test_small})
        print("‚úì Model accepts standard audio input shape")
        
        print("\n‚úÖ Final Verification Summary:")
        print("‚úì Model files present and accessible")
        print("‚úì ONNX model structure valid")
        print("‚úì Inference works correctly")
        print(f"‚úì Output matches PyTorch model (diff: {max_diff:.6f})")
        print(f"‚úì Model size appropriate ({model_size:.2f}MB)")
        print("‚úì Input processing verified")
        
        if max_diff < 1e-4 and model_size < 50:
            print("\nüéâ VERIFICATION SUCCESSFUL: Model is ready for deployment!")
        else:
            print("\n‚ö†Ô∏è VERIFICATION NOTE: Model works but may need optimization.")
            
    except Exception as e:
        print(f"\n‚ùå Error during verification: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # Update these paths to your model files
    onnx_path = "tiny.onnx"  # Path to your ONNX model
    pt_path = "tiny.pt"      # Path to your PyTorch model
    final_verification_check(onnx_path, pt_path)
